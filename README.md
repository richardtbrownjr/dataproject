# Data Scientist

### Education
University of North Carolina at Chapel Hill Concentration: Economics  
Wake Technical Community College: Data Science and Programming Support Services     

### Work Experience
Knox St. Studios
The North Carolina Institute of Minority Economic Development

### Projects
Data cleaning - Book List
Data Story Telling - TBA
Data Modeling - TBA

Data Cleaning
Objective:

Years ago, I discovered a collection of canonical works encompassing Greek, Roman, and American standards. This sparked the idea: What would constitute an African American canon of books? Drawing from my existing knowledge, I began compiling a list. Every so often I would add a few books. When I was thinking about a data portfolio I realized I had already started on  a data cleaning project.  The objective of this data cleaning project is to refine and standardize the dataset, which includes information about African American books spanning diverse genres such as history, art, and science. The dataset includes details like book title, genre, and author. The primary goal is to prepare the dataset for comprehensive analysis and exploration by addressing issues related to data integrity, consistency, and completeness.

Steps:

1. Data Assessment:
   - Conduct a thorough examination of the dataset to identify any missing values, inconsistencies, or inaccuracies. This involves checking for completeness in key fields such as publication year, genre, title, and author.

2. Genre Standardization:
   - Standardize the genre information to ensure consistency. This involves capitalization, eliminating extra spaces, and handling any variations or discrepancies in genre labels.

3. Author Name Unification:
   - Create a unified format for author names by combining the "First Name" and "Last Name" columns. Handle cases where either the first or last name is missing, ensuring a complete and standardized author field.

4.  Title Standardization:
   - Standardize book titles by capitalizing the first letter of each word, correcting any spelling mistakes, and ensuring a consistent format throughout the dataset.

5. Duplicate Identification:
   - Identify and handle any duplicate entries in the dataset. This involves comparing titles and author names to find potential duplicates and deciding on an appropriate action (e.g., merging or removing duplicates).

6. Documentation:
   - Maintain a comprehensive log documenting the decisions made during the data cleaning process, including any assumptions or adjustments. This documentation serves as a reference for reproducibility and transparency.

7. Publication Year Validation:
   - Validate and standardize publication years to ensure they follow a specific format and do not contain anomalies or outliers. Handle cases where the year is missing or not in a valid format.

Deliverables:
- A cleaned and standardized dataset ready for further analysis.
- Documentation detailing the steps taken, issues addressed, and any assumptions made during the data cleaning process.

This data cleaning project ensures that the African American books dataset is accurate, consistent, and well-prepared for subsequent analyses, providing a reliable foundation for further data mining.
